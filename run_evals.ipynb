{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare LLama Providers\n",
    "\n",
    "Run evaluations on a few prompts for llama3.1 70B across several providers and comprate results to baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and read in required packages, plus create an anthropic client.\n",
    "print('⏳ Installing packages')\n",
    "%pip install -q weave set-env-colab-kaggle-dotenv tqdm ipywidgets requests groq together\n",
    "print('✅ Packages installed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from set_env import set_env\n",
    "from openai import OpenAI\n",
    "from groq import Groq\n",
    "from together import Together\n",
    "import weave\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "set_env(\"OPENROUTER_API_KEY\")\n",
    "set_env(\"GROQ_API_KEY\")\n",
    "set_env(\"WANDB_API_KEY\")\n",
    "\n",
    "groqclient = Groq(\n",
    "    api_key=os.environ.get(\"GROQ_API_KEY\"),\n",
    ")\n",
    "client = Together(api_key=os.environ.get('TOGETHER_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Weave project\n",
    "weave.init('compare-llamas')\n",
    "\n",
    "\n",
    "class LlamaModel(weave.Model):\n",
    "    provider: str\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, prompt: str) -> str:\n",
    "        data = {\n",
    "            \"model\": \"meta-llama/llama-3.1-70b-instruct\",\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            \"temperature\": 0,\n",
    "            \"provider\": {\n",
    "                \"order\": [self.provider],\n",
    "                \"allow_fallbacks\": False\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        response = self.make_openrouter_request(data)\n",
    "        return response['choices'][0]['message']['content']\n",
    "\n",
    "    @weave.op()\n",
    "    def make_openrouter_request(self, data):\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                \"https://openrouter.ai/api/v1/chat/completions\",\n",
    "                headers = {\n",
    "                    \"Authorization\": f\"Bearer {os.environ['OPENROUTER_API_KEY']}\",\n",
    "                    \"Content-Type\": \"application/json\"\n",
    "                },\n",
    "                json=data\n",
    "            )\n",
    "            response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "            return response.json()\n",
    "        except requests.RequestException as e:\n",
    "            raise Exception(f\"API request failed: {str(e)}\")\n",
    "\n",
    "class GroqModel(weave.Model):\n",
    "\n",
    "    @weave.op()\n",
    "    def predict(self, prompt: str) -> str:\n",
    "        response = groqclient.chat.completions.create(\n",
    "            model='llama-3.1-70b-versatile',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            seed=123123\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    \n",
    "class TogetherModel(weave.Model):\n",
    "    @weave.op()\n",
    "    def predict(self, prompt: str) -> str:\n",
    "        response = self.make_together_request(prompt)\n",
    "        return response.choices[0].message.content\n",
    "\n",
    "    @weave.op()\n",
    "    def make_together_request(self, prompt):\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            temperature=0,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "# Create LLamas per provider OctoAI, NovitaAI, DeepInfra, Together, Fireworks\n",
    "octoai_llama = LlamaModel(provider='OctoAI', name='OctoAILLa_LLama3.1_70B')\n",
    "novitaai_llama = LlamaModel(provider='Novita', name='NovitaAI_LLaMa3.1_70B')\n",
    "deepinfra_llama = LlamaModel(provider='DeepInfra', name='DeepInfra_LLaMa3.1_70B')\n",
    "\n",
    "fireworks_llama = LlamaModel(provider='Fireworks', name='Fireworks_LLaMa3.1_70B')\n",
    "groq_llama = GroqModel(name='Groq_LLaMa3.1_70B')\n",
    "together_llama = TogetherModel(name='Together_LLaMa3.1_70B')\n",
    "\n",
    "print(\"✅ Weave models created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's build a dataset of quirky prompts and potentially their answers \n",
    "from weave import Dataset\n",
    "\n",
    "quirky_prompts = Dataset(\n",
    "    name=\"my_llama_quirky_prompts\",\n",
    "    rows=[\n",
    "        {\n",
    "            \"question\": \"Give me 10 sentences that end in the word \\\"apple\\\"\",\n",
    "            \"rubric\": \"all sentences must end with the word apple\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Answer with the number of legs about the following statement: The fox lost a leg, but then magically grew back the leg he lost and a mysterious extra leg on top of that\",\n",
    "            \"rubric\": \"Answer must be 5 or five\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Yam (a boy) has 4 sisters. Each sister has 3 brothers. How many brothers does Yam have? Let's think step by step.\",\n",
    "            \"rubric\": \"Answer must indicate that Yam has 2 brothers\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"You have five apples today, you ate two apples yesterday so how many apples do you have today? Provide a logical answer.\",\n",
    "            \"rubric\": \"Answer must be five and explain that yesterdays action have no bearing on todays apple quantity\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"Which number is bigger: 9.11 or 9.9?\",\n",
    "            \"rubric\": \"Answer should conclude that 9.9 is bigger\" \n",
    "        },\n",
    "        {\n",
    "            \"question\": \"If I hang 5 shirts outside and it takes them 5 hours to dry, how long would it take to dry 30 shirts\",\n",
    "            \"rubric\": \"Answer must state that it would take the same amount of time\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"There are three sisters in a room. Anna is reading a book. Alice is playing a match of chess against someone in the room. What is the third sister, Amanda, doing?\",\n",
    "            \"rubric\": \"Playing chess with Alice\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"\"\"Determine all triples (x, y, z) of real numbers that are solutions to the following\n",
    "system of equations:\n",
    "log9 x + 10g9 y + 10g3 z = 2\n",
    "log 16 x + log4 y + log16 z = 1\n",
    "log5 x + log25 y + log25 z = 0\n",
    "\"\"\",\n",
    "            \"rubric\": \"IDK the answer to this one\"\n",
    "        }\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all instantiated models\n",
    "# models = [octoai_llama, together_llama]\n",
    "models = [deepinfra_llama, fireworks_llama, groq_llama]\n",
    "\n",
    "\n",
    "#define our scoring functions\n",
    "@weave.op()\n",
    "def has_response(rubric: str, model_output: dict) -> dict:\n",
    "    return {'has_response': model_output is not None}\n",
    "\n",
    "# Define the preprocess_model_input function\n",
    "def preprocess_model_input(row):\n",
    "    return {'prompt': row['question']}\n",
    "\n",
    "\n",
    "# Define the evaluation\n",
    "evaluation = weave.Evaluation(\n",
    "    name='quirky_prompts_eval',\n",
    "    dataset=quirky_prompts,\n",
    "    trials=1,\n",
    "    scorers=[\n",
    "        has_response\n",
    "    ],\n",
    "    preprocess_model_input=preprocess_model_input\n",
    ")\n",
    "\n",
    "# Run evaluation for each model\n",
    "results = {}\n",
    "for model in models:\n",
    "    print(f\"Evaluating {model.name}...\")\n",
    "    result = await evaluation.evaluate(model)\n",
    "    results[model.name] = result\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comparellamas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
